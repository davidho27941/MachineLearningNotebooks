{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development with Custom Weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to retrain a model with custom weights and fine-tune the model with quantization, then deploy the model running on FPGA. Only Windows is supported. We use TensorFlow and Keras to build our model. We are going to use transfer learning, with ResNet50 as a featurizer. We don't use the last layer of ResNet50 in this case and instead add our own classification layer using Keras.\n",
    "\n",
    "The custom wegiths are trained with ImageNet on ResNet50. We are using a public Top tagging dataset as our training data.\n",
    "\n",
    "Please set up your environment as described in the [quick start](project-brainwave-quickstart.ipynb).\n",
    "\n",
    "This work was performed on the Caltech GPU cluster. The specific server is named imperium-sm.hep.caltech.edu. Paths have been set to work in that environment, but must be altered for your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "After you train your model in float32, you'll write the weights to a place on disk. We also need a location to store the models that get downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These directories were chosen because they write the data to local disk, which will have the fastest access time\n",
    "# of our various storage options.\n",
    "custom_weights_dir = os.path.expanduser(\"/data/shared/dwerran/custom-weights-retrain/weights\")\n",
    "saved_model_dir = os.path.expanduser(\"/data/shared/dwerran/custom-weights-retrain/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "Load the files we are going to use for training and testing. The public Top dataset consists of image formatted data, but our data has been preprocessed into a raw form.\n",
    "\n",
    "At the time of writing, the files in question are located at `/data/shared/dwerran/converted`. They are stored in the HDF5 format, and must be accessed via the `tables` module. The two sub-datasets we're interested in are `/img-pt` and `/labels`, corresponding to the images and lables respectively. Each dataset contains 50000 images, and there are about 30 datasets. As before, this storage location was chosen to maximize data bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_rgb(images): \n",
    "    #normalize image to 0-255 per image.\n",
    "    image_sum = 1/np.sum(np.sum(images,axis=1),axis=-1)\n",
    "    given_axis = 0\n",
    "    # Create an array which would be used to reshape 1D array, b to have \n",
    "    # singleton dimensions except for the given axis where we would put -1 \n",
    "    # signifying to use the entire length of elements along that axis  \n",
    "    dim_array = np.ones((1,images.ndim),int).ravel()\n",
    "    dim_array[given_axis] = -1\n",
    "    # Reshape b with dim_array and perform elementwise multiplication with \n",
    "    # broadcasting along the singleton dimensions for the final output\n",
    "    image_sum_reshaped = image_sum.reshape(dim_array)\n",
    "    images = images*image_sum_reshaped*255\n",
    "\n",
    "    # make it rgb by duplicating 3 channels.\n",
    "    images = np.stack([image, image, image],axis=-1)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "datadir = \"/data/shared/dwerran/converted/\"\n",
    "num_train = 10000  # Limit the number of images used in training to shorten epoch time\n",
    "\n",
    "train_files = glob.glob(os.path.join(datadir, 'train_file_*'))\n",
    "train_files = random.choice(train_files)  # Choose one of the training files to use (for now)\n",
    "\n",
    "# Open the chosen file and extract the dataset\n",
    "f = tables.open_file(train_files, 'r')\n",
    "a = np.array(f.root.img_pt) # Images\n",
    "b = np.array(f.root.labels) # Labels\n",
    "# Randomly shuffle label and images, keep the indexing\n",
    "c = np.c_[a.reshape(len(a), -1), b.reshape(len(b), -1)]\n",
    "np.random.shuffle(c)\n",
    "train_images = c[:, :a.size//len(a)].reshape(a.shape)\n",
    "train_labels = c[:, a.size//len(a):].reshape(b.shape)\n",
    "\n",
    "# Limit the data set to make the notebook execute quickly.\n",
    "train_images = train_images[:num_train]\n",
    "train_labels = train_labels[:num_train]\n",
    "\n",
    "train_images = normalize_and_rgb(train_images)\n",
    "train_images = tf.convert_to_tensor(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = 1000\n",
    "\n",
    "val_files = glob.glob(os.path.join(datadir, 'val_file_*'))\n",
    "val_files = random.choice(test_files)  # Choose one of the training files to use (for now)\n",
    "\n",
    "# Open the chosen file and extract the dataset\n",
    "f = tables.open_file(val_files, 'r')\n",
    "a = np.array(f.root.img_pt) # Images\n",
    "b = np.array(f.root.labels) # Labels\n",
    "# Randomly shuffle label and images, keep the indexing\n",
    "c = np.c_[a.reshape(len(a), -1), b.reshape(len(b), -1)]\n",
    "np.random.shuffle(c)\n",
    "val_images = c[:, :a.size//len(a)].reshape(a.shape)\n",
    "val_labels = c[:, a.size//len(a):].reshape(b.shape)\n",
    "\n",
    "# Limit the data set to make the notebook execute quickly.\n",
    "val_images = val_images[:num_val]\n",
    "val_labels = val_labels[:num_val]\n",
    "\n",
    "val_images = normalize_and_rgb(val_images)\n",
    "val_images = tf.convert_to_tensor(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model\n",
    "We use ResNet50 for the featuirzer and build our own classifier using Keras layers. We train the featurizer and the classifier as one model. The weights trained on ImageNet are used as the starting point for the retraining of our featurizer. The weights are loaded from tensorflow checkpoint files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing image dataset to the ResNet50 featurizer, we need to preprocess the input file to get it into the form expected by ResNet50. ResNet50 expects float tensors representing the images in BGR, channel last order. Given that our images are greyscale, this isn't relevant to us, as we will simply be copying the data in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.contrib.brainwave.models.utils as utils\n",
    "\n",
    "def preprocess_images():\n",
    "    # Create a placeholder for our incoming images\n",
    "    in_images = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Resize those images to fit our featurizer\n",
    "    output_width = 224\n",
    "    output_height = 224\n",
    "    in_images = tf.image.resize_images(in_images, [output_height,output_wdith])\n",
    "    in_images.set_shape([output_height, output_width, 3])\n",
    "    in_images = tf.to_float(in_images)\n",
    "    in_images = tf.expand_dims(in_images, 0)\n",
    "    \n",
    "    # Convert images to 3D tensors [width,height,channel] - channels are in BGR order.\n",
    "    image_tensors = utils.preprocess_array(in_images)\n",
    "    return in_images, image_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Keras layer APIs to construct the classifier. Because we're using the tensorflow backend, we can train this classifier in one session with our Resnet50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_classifier(in_tensor):\n",
    "    from keras.layers import Dropout, Dense, Flatten\n",
    "    K.set_session(tf.get_default_session())\n",
    "    \n",
    "    FC_SIZE = 1024\n",
    "    NUM_CLASSES = 2\n",
    "\n",
    "    x = Dropout(0.2, input_shape=(1, 1, 2048,))(in_tensor)\n",
    "    x = Dense(FC_SIZE, activation='relu', input_dim=(1, 1, 2048,))(x)\n",
    "    x = Flatten()(x)\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax', input_dim=FC_SIZE, name='classifier_output')(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every component of the model is defined, we can construct the model. Constructing the model with the project brainwave models is two steps - first we import the graph definition, then we restore the weights of the model into a tensorflow session. Because the quantized graph defintion and the float32 graph defintion share the same node names in the graph definitions, we can initally train the weights in float32, and then reload them with the quantized operations (which take longer) to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(quantized, starting_weights_directory = None):\n",
    "    from azureml.contrib.brainwave.models import Resnet50, QuantizedResnet50\n",
    "    \n",
    "    # Convert images to 3D tensors [width,height,channel]\n",
    "    in_images, image_tensors = preprocess_images()\n",
    "\n",
    "    # Construct featurizer using quantized or unquantized ResNet50 model\n",
    "    if not quantized:\n",
    "        featurizer = Resnet50(saved_model_dir)\n",
    "    else:\n",
    "        featurizer = QuantizedResnet50(saved_model_dir, custom_weights_directory = starting_weights_directory)\n",
    "\n",
    "\n",
    "    features = featurizer.import_graph_def(input_tensor=image_tensors)\n",
    "    # Construct classifier\n",
    "    preds = construct_classifier(features)\n",
    "    \n",
    "    # Initialize weights\n",
    "    sess = tf.get_default_session()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    featurizer.restore_weights(sess)\n",
    "\n",
    "    return in_images, image_tensors, features, preds, featurizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "First we train the model with custom weights but without quantization. Training is done with native float precision (32-bit floats). We load the traing data set and batch the training with 10 epochs. When the performance reaches desired level or starts decredation, we stop the training iteration and save the weights as tensorflow checkpoint files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(files):\n",
    "    \"\"\" Read files to array\"\"\"\n",
    "    contents = []\n",
    "    for path in files:\n",
    "        with open(path, 'rb') as f:\n",
    "            contents.append(f.read())\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(preds, in_images, img_train, label_train, is_retrain = False, train_epoch = 10):\n",
    "    \"\"\" training model \"\"\"\n",
    "    from keras.objectives import binary_crossentropy\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    learning_rate = 0.001 if is_retrain else 0.01\n",
    "        \n",
    "    # Specify the loss function\n",
    "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))   \n",
    "    cross_entropy = tf.reduce_mean(binary_crossentropy(in_labels, preds))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    def chunks(a, b, n):\n",
    "        \"\"\"Yield successive n-sized chunks from a and b.\"\"\"\n",
    "        if (len(a) != len(b)):\n",
    "            print(\"a and b are not equal in chunks(a,b,n)\")\n",
    "            raise ValueError(\"Parameter error\")\n",
    "\n",
    "        for i in range(0, len(a), n):\n",
    "            yield a[i:i + n], b[i:i + n]\n",
    "\n",
    "    chunk_size = 16\n",
    "    chunk_num = len(label_train) / chunk_size\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    for epoch in range(train_epoch):\n",
    "        avg_loss = 0\n",
    "        for img_chunk, label_chunk in tqdm(chunks(img_train, label_train, chunk_size)):\n",
    "            contents = read_files(img_chunk)\n",
    "            _, loss = sess.run([optimizer, cross_entropy],\n",
    "                                feed_dict={in_images: contents,\n",
    "                                           in_labels: label_chunk,\n",
    "                                           K.learning_phase(): 1})\n",
    "            avg_loss += loss / chunk_num\n",
    "        print(\"Epoch:\", (epoch + 1), \"loss = \", \"{:.3f}\".format(avg_loss))\n",
    "            \n",
    "        # Reach desired performance\n",
    "        if (avg_loss < 0.001):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(preds, in_images, img_test, label_test):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    from keras.metrics import categorical_accuracy\n",
    "\n",
    "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    accuracy = tf.reduce_mean(categorical_accuracy(in_labels, preds))\n",
    "    contents = read_files(img_test)\n",
    "\n",
    "    accuracy = accuracy.eval(feed_dict={in_images: contents,\n",
    "                                        in_labels: label_test,\n",
    "                                        K.learning_phase(): 0})\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the training\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    in_images, image_tensors, features, preds, featurizer = construct_model(quantized=False)\n",
    "    train_model(preds, in_images, img_train, label_train, is_retrain=False, train_epoch=10)    \n",
    "    accuracy = test_model(preds, in_images, img_test, label_test)  \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    featurizer.save_weights(custom_weights_dir + \"/rn50\", tf.get_default_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "After training, we evaluate the trained model's accuracy on test dataset with quantization. So that we know the model's performance if it is deployed on the FPGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Testing trained model with quantization\")\n",
    "    in_images, image_tensors, features, preds, quantized_featurizer = construct_model(quantized=True, starting_weights_directory=custom_weights_dir)\n",
    "    accuracy = test_model(preds, in_images, img_test, label_test)      \n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Model\n",
    "Sometimes, the model's accuracy can drop significantly after quantization. In those cases, we need to retrain the model enabled with quantization to get better model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (accuracy < 0.93):\n",
    "    with sess.as_default():\n",
    "        print(\"Fine-tuning model with quantization\")\n",
    "        train_model(preds, in_images, img_train, label_train, is_retrain=True, train_epoch=10)\n",
    "        accuracy = test_model(preds, in_images, img_test, label_test)        \n",
    "        print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Definition\n",
    "Like in the QuickStart notebook our service definition pipeline consists of three stages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.brainwave.pipeline import ModelDefinition, TensorflowStage, BrainWaveStage\n",
    "\n",
    "model_def_path = os.path.join(saved_model_dir, 'model_def.zip')\n",
    "\n",
    "model_def = ModelDefinition()\n",
    "model_def.pipeline.append(TensorflowStage(sess, in_images, image_tensors))\n",
    "model_def.pipeline.append(BrainWaveStage(sess, quantized_featurizer))\n",
    "model_def.pipeline.append(TensorflowStage(sess, features, preds))\n",
    "model_def.save(model_def_path)\n",
    "print(model_def_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "Go to our [GitHub repo](https://aka.ms/aml-real-time-ai) \"docs\" folder to learn how to create a Model Management Account and find the required information below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time the code below runs it will create a new service running your model. If you want to change the model you can make changes above in this notebook and save a new service definition. Then this code will update the running service in place to run the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.core.image import Image\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.contrib.brainwave import BrainwaveWebservice, BrainwaveImage\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "model_name = \"catsanddogs-resnet50-model\"\n",
    "image_name = \"catsanddogs-resnet50-image\"\n",
    "service_name = \"modelbuild-service\"\n",
    "\n",
    "registered_model = Model.register(ws, model_def_path, model_name)\n",
    "\n",
    "image_config = BrainwaveImage.image_configuration()\n",
    "deployment_config = BrainwaveWebservice.deploy_configuration()\n",
    "    \n",
    "try:\n",
    "    service = Webservice(ws, service_name)\n",
    "    service.delete()\n",
    "    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n",
    "    service.wait_for_deployment(True)\n",
    "except WebserviceException:\n",
    "    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n",
    "    service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service is now running in Azure and ready to serve requests. We can check the address and port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.ipAddress + ':' + str(service.port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client\n",
    "There is a simple test client at amlrealtimeai.PredictionClient which can be used for testing. We'll use this client to score an image with our new service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.brainwave.client import PredictionClient\n",
    "client = PredictionClient(service.ipAddress, service.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adapt the client [code](../../pythonlib/amlrealtimeai/client.py) to meet your needs. There is also an example C# [client](../../sample-clients/csharp).\n",
    "\n",
    "The service provides an API that is compatible with TensorFlow Serving. There are instructions to download a sample client [here](https://www.tensorflow.org/serving/setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request\n",
    "Let's see how our service does on a few images. It may get a few wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an image to classify\n",
    "print('CATS')\n",
    "for image_file in cat_files[:8]:\n",
    "    results = client.score_image(image_file)\n",
    "    result = 'CORRECT ' if results[0] > results[1] else 'WRONG '\n",
    "    print(result + str(results))\n",
    "print('DOGS')\n",
    "for image_file in dog_files[:8]:\n",
    "    results = client.score_image(image_file)\n",
    "    result = 'CORRECT ' if results[1] > results[0] else 'WRONG '\n",
    "    print(result + str(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Run the cell below to delete your service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License for plot_confusion_matrix:\n",
    "\n",
    "New BSD License\n",
    "\n",
    "Copyright (c) 2007-2018 The scikit-learn developers.\n",
    "All rights reserved.\n",
    "\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "  a. Redistributions of source code must retain the above copyright notice,\n",
    "     this list of conditions and the following disclaimer.\n",
    "  b. Redistributions in binary form must reproduce the above copyright\n",
    "     notice, this list of conditions and the following disclaimer in the\n",
    "     documentation and/or other materials provided with the distribution.\n",
    "  c. Neither the name of the Scikit-learn Developers  nor the names of\n",
    "     its contributors may be used to endorse or promote products\n",
    "     derived from this software without specific prior written\n",
    "     permission. \n",
    "\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR\n",
    "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n",
    "LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n",
    "OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n",
    "DAMAGE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "coverste"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
