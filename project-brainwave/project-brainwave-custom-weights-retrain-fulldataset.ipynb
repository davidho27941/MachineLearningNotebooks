{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development with Custom Weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to retrain a model with custom weights and fine-tune the model with quantization, then deploy the model running on FPGA. Only Windows is supported. We use TensorFlow and Keras to build our model. We are going to use transfer learning, with ResNet50 as a featurizer. We don't use the last layer of ResNet50 in this case and instead add our own classification layer using Keras.\n",
    "\n",
    "The custom wegiths are trained with ImageNet on ResNet50. We are using a public Top tagging dataset as our training data.\n",
    "\n",
    "Please set up your environment as described in the [quick start](project-brainwave-quickstart.ipynb).\n",
    "\n",
    "This work was performed on the Caltech GPU cluster. The specific server is named imperium-sm.hep.caltech.edu. Paths have been set to work in that environment, but must be altered for your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6059030616646121732, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 15577998951\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 11022026294613813982\n",
       " physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.backend import manual_variable_initialization\n",
    "manual_variable_initialization(True)\n",
    "import tables\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "After you train your model in float32, you'll write the weights to a place on disk. We also need a location to store the models that get downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These directories were chosen because they write the data to local disk, which will have the fastest access time\n",
    "# of our various storage options.\n",
    "custom_weights_dir = os.path.expanduser(\"../machinelearningnotebooks/weights-floatingpoint\")\n",
    "custom_weights_dir_q = os.path.expanduser(\"../weights-quantized\")\n",
    "custom_weights_dir_tl = os.path.expanduser(\"../weights-transferlearning-floatingpoint\")\n",
    "saved_model_dir = os.path.expanduser(\"../machinelearningnotebooks/models\")\n",
    "results_dir = os.path.expanduser(\"../results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "Load the files we are going to use for training and testing. The public Top dataset consists of image formatted data, but our data has been preprocessed into a raw form.\n",
    "\n",
    "At the time of writing, the files in question are located at `/data/shared/dwerran/converted`. They are stored in the HDF5 format, and must be accessed via the `tables` module. The two sub-datasets we're interested in are `/img-pt` and `/labels`, corresponding to the images and lables respectively. Each dataset contains 50000 images, and there are about 30 datasets. As before, this storage location was chosen to maximize data bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize_and_rgb, image_with_label, count_events, save_results, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train_events = 1211000\n",
      "n_test_events = 404000\n",
      "n_val_events = 404000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "datadir = \"../machinelearningnotebooks/data/\"\n",
    "n_train_file = 25\n",
    "n_test_file = 9\n",
    "n_val_file = 9\n",
    "\n",
    "train_files = glob.glob(os.path.join(datadir, 'train_file_*'))\n",
    "test_files = glob.glob(os.path.join(datadir, 'test/test_file_*'))\n",
    "val_files = glob.glob(os.path.join(datadir, 'val_file_*'))\n",
    "train_files = train_files[:n_train_file]\n",
    "test_files = test_files[:n_test_file]\n",
    "val_files = test_files[:n_val_file]\n",
    "\n",
    "n_train_events = count_events(train_files)\n",
    "n_test_events = count_events(test_files)\n",
    "n_val_events = count_events(val_files)\n",
    "\n",
    "print(\"n_train_events =\", n_train_events)\n",
    "print(\"n_test_events =\", n_test_events)\n",
    "print(\"n_val_events =\", n_val_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model\n",
    "We use ResNet50 for the featuirzer and build our own classifier using Keras layers. We train the featurizer and the classifier as one model. The weights trained on ImageNet are used as the starting point for the retraining of our featurizer. The weights are loaded from tensorflow checkpoint files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing image dataset to the ResNet50 featurizer, we need to preprocess the input file to get it into the form expected by ResNet50. ResNet50 expects float tensors representing the images in BGR, channel last order. Given that our images are greyscale, this isn't relevant to us, as we will simply be copying the data in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Keras layer APIs to construct the classifier. Because we're using the tensorflow backend, we can train this classifier in one session with our Resnet50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import construct_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every component of the model is defined, we can construct the model. Constructing the model with the project brainwave models is two steps - first we import the graph definition, then we restore the weights of the model into a tensorflow session. Because the quantized graph defintion and the float32 graph defintion share the same node names in the graph definitions, we can initally train the weights in float32, and then reload them with the quantized operations (which take longer) to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import construct_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "First we train the model with custom weights but without quantization. Training is done with native float precision (32-bit floats). We load the traing data set and batch the training with 10 epochs. When the performance reaches desired level or starts decredation, we stop the training iteration and save the weights as tensorflow checkpoint files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import chunks, train_model, test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training currently leverages a hack to work around some apparent limits in the BW API. I have attempted to specify a custom weights directory when calling the `Resnet50` function in `construct_model()` above in the same way it is specified for `Quantized_Resnet50`. However, this throws an error, and since there is no API documentation yet, the way I'm working around it is rewriting our trained weights to the saved model directory. I will be reaching out to the team on this topic to see if they have a better suggestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in restore_weights\n",
      "checkpoint directory: ../machinelearningnotebooks/weights-floatingpoint\n",
      "lastest checkpoint: ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "INFO:tensorflow:Restoring parameters from ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "loading classifier weights from ../machinelearningnotebooks/weights-floatingpoint/class_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [03:28<00:00,  4.84it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss =  0.161\n",
      "Training Accuracy: 0.934 , Area under ROC curve: 0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:09<00:00, 22.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 val_loss =  0.174\n",
      "Validation Accuracy: 0.930 , Area under ROC curve: 0.983\n",
      "new best model\n",
      "in restore_weights\n",
      "checkpoint directory: ../machinelearningnotebooks/weights-floatingpoint\n",
      "lastest checkpoint: ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "INFO:tensorflow:Restoring parameters from ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "loading classifier weights from ../machinelearningnotebooks/weights-floatingpoint/class_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6319it [08:15, 12.75it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss =  0.176\n",
      "Test Accuracy: 0.930 , Area under ROC curve: 0.981\n"
     ]
    }
   ],
   "source": [
    "# Launch the training\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "num_epoch_train = 1\n",
    "\n",
    "with sess.as_default():\n",
    "    in_images, image_tensors, features, preds, featurizer, classifier = construct_model(quantized=False, saved_model_dir=saved_model_dir, starting_weights_directory=custom_weights_dir, is_training=True)\n",
    "    # It's necessary to specify global (all) variables when using the saver in this instance.\n",
    "    # Since we are using batch norm layers, whose variables aren't saved by default, we\n",
    "    # include them this way.\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep = 100)\n",
    "    loss_over_epoch, accuracy_over_epoch, auc_over_epoch, val_loss_over_epoch, val_accuracy_over_epoch, val_auc_over_epoch = \\\n",
    "        train_model(preds, in_images, train_files[:1], val_files[:1], is_retrain=True, train_epoch=num_epoch_train, \n",
    "                    classifier=classifier,\n",
    "                    saver=saver, checkpoint_path=custom_weights_dir) \n",
    "    _, _, features, preds, featurizer, classifier = construct_model(quantized=False, saved_model_dir=saved_model_dir, starting_weights_directory=custom_weights_dir, is_training=False)\n",
    "    loss, accuracy, auc, preds_test, test_labels = test_model(preds, in_images, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Model\n",
    "Here, we re-load the weights saved on disk and test the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a trained model\n",
      "in restore_weights\n",
      "checkpoint directory: ../machinelearningnotebooks/weights-floatingpoint\n",
      "lastest checkpoint: ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "INFO:tensorflow:Restoring parameters from ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "loading classifier weights from ../machinelearningnotebooks/weights-floatingpoint/class_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6319it [07:56, 13.25it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss =  0.285\n",
      "Test Accuracy: 0.889 , Area under ROC curve: 0.955\n",
      "Accuracy: 0.8891262376236997 , Area under ROC curve: 0.9550782772810158\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Loading a trained model\")\n",
    "    in_images, image_tensors, features, preds, featurizer, classifier = construct_model(quantized=False, saved_model_dir=saved_model_dir, starting_weights_directory=custom_weights_dir, is_training=False)\n",
    "    loss, accuracy, auc, preds_test, test_labels = test_model(preds, in_images, test_files)\n",
    "    print(\"Accuracy:\", accuracy, \", Area under ROC curve:\", auc)\n",
    "    \n",
    "# Call the save results utility.\n",
    "save_results(results_dir, 't', accuracy, test_labels, preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Quantized Model\n",
    "After training, we evaluate the trained model's accuracy on test dataset with quantization. So that we know the model's performance if it is deployed on the FPGA. The only significant difference between this cell and the cell two below is this loads weights from the floating point directory and the one two below loads from the quantized directory. In this way, you can compare pre- and post- quantization fine tuning tests.\n",
    "\n",
    "It's been found that an abysmal score here does not necessarily reflect a broken model. Quantization has very negative effects on the batch normalization layers, and should quickly be corrected in only a few epochs of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained model with quantization\n",
      "in restore_weights\n",
      "checkpoint directory: ../machinelearningnotebooks/weights-floatingpoint\n",
      "lastest checkpoint: ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "INFO:tensorflow:Restoring parameters from ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "loading classifier weights from ../machinelearningnotebooks/weights-floatingpoint/class_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6319it [35:11,  3.81it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss =  8.019\n",
      "Test Accuracy: 0.500 , Area under ROC curve: 0.499\n",
      "Accuracy: 0.4997871287128282 , Area under ROC curve: 0.4985320567081477\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Testing trained model with quantization\")\n",
    "    in_images_q, image_tensors_q, features_q, preds_q, quantized_featurizer, classifier = construct_model(quantized=True, saved_model_dir=saved_model_dir, starting_weights_directory=custom_weights_dir, is_training=False)\n",
    "    loss_q, accuracy_q, auc_q, preds_test_q, test_labels_q = test_model(preds_q, in_images_q, test_files)\n",
    "    print(\"Accuracy:\", accuracy_q, \", Area under ROC curve:\", auc_q)\n",
    "    \n",
    "# Call the save results utility.\n",
    "save_results(results_dir, 'q', accuracy_q, test_labels_q, preds_test_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Quantized Model\n",
    "Sometimes, the model's accuracy can drop significantly after quantization. In those cases, we need to retrain the model enabled with quantization to get better model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with quantization\n",
      "in restore_weights\n",
      "checkpoint directory: ../machinelearningnotebooks/weights-floatingpoint\n",
      "lastest checkpoint: ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "INFO:tensorflow:Restoring parameters from ../machinelearningnotebooks/weights-floatingpoint/resnet50_bw_best\n",
      "loading classifier weights from ../machinelearningnotebooks/weights-floatingpoint/class_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [08:11<00:00,  2.76it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss =  0.175\n",
      "Training Accuracy: 0.930 , Area under ROC curve: 0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:19<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 val_loss =  0.184\n",
      "Validation Accuracy: 0.926 , Area under ROC curve: 0.981\n",
      "new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 3092/6313 [17:34<17:54,  3.00it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a792ead4a06c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     saver=saver, checkpoint_path=custom_weights_dir_q) \n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_test_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MachineLearningNotebooks/project-brainwave/utils.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(preds, in_images, test_files)\u001b[0m\n\u001b[1;32m    356\u001b[0m                         feed_dict={in_images: img_chunk,\n\u001b[1;32m    357\u001b[0m                                    \u001b[0min_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_chunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                                    K.learning_phase(): 0})\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mavg_test_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreal_chunk_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_test_events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mavg_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_result\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreal_chunk_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_test_events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jduarte/miniconda3/envs/myamlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jduarte/miniconda3/envs/myamlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jduarte/miniconda3/envs/myamlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jduarte/miniconda3/envs/myamlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jduarte/miniconda3/envs/myamlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jduarte/miniconda3/envs/myamlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "num_epoch_finetune = 1\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Fine-tuning model with quantization\")        \n",
    "    \n",
    "    in_images, image_tensors, features, preds, quantized_featurizer, classifier = construct_model(quantized=True, saved_model_dir=saved_model_dir, starting_weights_directory=custom_weights_dir, is_training=True)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep = 100)\n",
    "    loss_over_epoch_ft, accuracy_over_epoch_ft, auc_over_epoch_ft, val_loss_over_epoch_ft, val_accuracy_over_epoch_ft, val_auc_over_epoch_ft = \\\n",
    "        train_model(preds, in_images, train_files[2:3], val_files[2:3], is_retrain=True, train_epoch=num_epoch_finetune, \n",
    "                    classifier=classifier,\n",
    "                    saver=saver, checkpoint_path=custom_weights_dir_q) \n",
    "    loss_ft, accuracy_ft, auc_ft, preds_test_ft, test_labels_ft = test_model(preds, in_images, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Quantized Model\n",
    "After training, we evaluate the trained model's accuracy on test dataset with quantization. So that we know the model's performance if it is deployed on the FPGA. The only significant difference between this cell and the cell two above is this loads weights from the quantized directory and the one two above loads from the floating point directory. In this way, you can compare pre- and post- quantization fine tuning tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-load fine-tuned model with quantization\n",
      "in restore_weights\n",
      "checkpoint directory: ../weights-quantized\n",
      "lastest checkpoint: ../weights-quantized/resnet50_bw\n",
      "INFO:tensorflow:Restoring parameters from ../weights-quantized/resnet50_bw\n",
      "loading classifier weights from ../weights-quantized/class_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 6279/6313 [34:52<00:11,  3.04it/s]  "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "with sess.as_default():\n",
    "    print(\"Re-load fine-tuned model with quantization\")   \n",
    "    in_images_q, image_tensors_q, features_q, preds_q, quantized_featurizer, classifier = construct_model(quantized=True, saved_model_dir=saved_model_dir, starting_weights_directory=custom_weights_dir_q, is_training=False)\n",
    "    loss_ft, accuracy_ft, auc_ft, preds_test_ft, test_labels_ft = test_model(preds_q, in_images_q, test_files)\n",
    "    \n",
    "# Call the save results utility.\n",
    "save_results(results_dir, 'ft', accuracy_ft, test_labels_ft, preds_test_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "num_epoch_train = 20\n",
    "\n",
    "with sess.as_default():\n",
    "    in_images, image_tensors, features, preds, featurizer, classifier = construct_model(quantized=False, is_frozen=True, saved_model_dir=saved_model_dir)\n",
    "    saver = tf.train.Saver(max_to_keep = 100)\n",
    "    loss_over_epoch_tl, accuracy_over_epoch_tl, auc_over_epoch_tl, val_loss_over_epoch_tl, val_accuracy_over_epoch_tl, val_auc_over_epoch_tl = \\\n",
    "        train_model(preds, in_images, train_files, val_files, is_retrain=True, train_epoch=num_epoch_train, \n",
    "                    classifier=classifier,\n",
    "                    saver=saver, checkpoint_path=custom_weights_dir_tl) \n",
    "    loss_tl, accuracy_tl, auc_tl, preds_test_tl, test_labels_tl = test_model(preds, in_images, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a training\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Loading a trained model\")\n",
    "    in_images, image_tensors, features, preds, featurizer, classifier = construct_model(quantized=False, is_frozen=True, saved_model_dir=saved_model_dir)\n",
    "    print(\"loading classifier weights from\", custom_weights_dir_tl+'/class_weights.h5')\n",
    "    classifier.load_weights(custom_weights_dir_tl+'/class_weights.h5')\n",
    "    loss_tl, accuracy_tl, auc_tl, preds_test_tl, test_labels_tl = test_model(preds, in_images, test_files)\n",
    "    print(\"Accuracy:\", accuracy_tl, \", Area under ROC curve:\", auc_tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Quantized Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Re-load fine-tuned model with quantization\")   \n",
    "    in_images_q, image_tensors_q, features_q, preds_q, quantized_featurizer, classifier = construct_model(quantized=True, is_frozen=True, saved_model_dir=saved_model_dir)\n",
    "    print(\"loading classifier weights from\", custom_weights_dir_tl+'/class_weights.h5')\n",
    "    classifier.load_weights(custom_weights_dir_tl+'/class_weights.h5')\n",
    "    loss_tl_q, accuracy_tl_q, auc_tl_q, preds_test_tl_q, test_labels_tl_q = test_model(preds_q, in_images_q, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_results, plot_results\n",
    "save_results(results_dir, 't', test_labels, preds_test)\n",
    "plot_results(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License for plot_confusion_matrix:\n",
    "\n",
    "New BSD License\n",
    "\n",
    "Copyright (c) 2007-2018 The scikit-learn developers.\n",
    "All rights reserved.\n",
    "\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "  a. Redistributions of source code must retain the above copyright notice,\n",
    "     this list of conditions and the following disclaimer.\n",
    "  b. Redistributions in binary form must reproduce the above copyright\n",
    "     notice, this list of conditions and the following disclaimer in the\n",
    "     documentation and/or other materials provided with the distribution.\n",
    "  c. Neither the name of the Scikit-learn Developers  nor the names of\n",
    "     its contributors may be used to endorse or promote products\n",
    "     derived from this software without specific prior written\n",
    "     permission. \n",
    "\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR\n",
    "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n",
    "LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n",
    "OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n",
    "DAMAGE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "coverste"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
