{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development with Custom Weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to retrain a model with custom weights and fine-tune the model with quantization, then deploy the model running on FPGA. Only Windows is supported. We use TensorFlow and Keras to build our model. We are going to use transfer learning, with ResNet50 as a featurizer. We don't use the last layer of ResNet50 in this case and instead add our own classification layer using Keras.\n",
    "\n",
    "The custom wegiths are trained with ImageNet on ResNet50. We are using a public Top tagging dataset as our training data.\n",
    "\n",
    "Please set up your environment as described in the [quick start](project-brainwave-quickstart.ipynb).\n",
    "\n",
    "This work was performed on the Caltech GPU cluster. The specific server is named imperium-sm.hep.caltech.edu. Paths have been set to work in that environment, but must be altered for your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import setGPU\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "After you train your model in float32, you'll write the weights to a place on disk. We also need a location to store the models that get downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These directories were chosen because they write the data to local disk, which will have the fastest access time\n",
    "# of our various storage options.\n",
    "custom_weights_dir = os.path.expanduser(\"/data/shared/dwerran/custom-weights-retrain/weights\")\n",
    "saved_model_dir = os.path.expanduser(\"/data/shared/dwerran/custom-weights-retrain/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "Load the files we are going to use for training and testing. The public Top dataset consists of image formatted data, but our data has been preprocessed into a raw form.\n",
    "\n",
    "At the time of writing, the files in question are located at `/data/shared/dwerran/converted`. They are stored in the HDF5 format, and must be accessed via the `tables` module. The two sub-datasets we're interested in are `/img-pt` and `/labels`, corresponding to the images and lables respectively. Each dataset contains 50000 images, and there are about 30 datasets. As before, this storage location was chosen to maximize data bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_rgb(images): \n",
    "    #normalize image to 0-255 per image.\n",
    "    image_sum = 1/np.sum(np.sum(images,axis=1),axis=-1)\n",
    "    given_axis = 0\n",
    "    # Create an array which would be used to reshape 1D array, b to have \n",
    "    # singleton dimensions except for the given axis where we would put -1 \n",
    "    # signifying to use the entire length of elements along that axis  \n",
    "    dim_array = np.ones((1,images.ndim),int).ravel()\n",
    "    dim_array[given_axis] = -1\n",
    "    # Reshape b with dim_array and perform elementwise multiplication with \n",
    "    # broadcasting along the singleton dimensions for the final output\n",
    "    image_sum_reshaped = image_sum.reshape(dim_array)\n",
    "    images = images*image_sum_reshaped*255\n",
    "\n",
    "    # make it rgb by duplicating 3 channels.\n",
    "    images = np.stack([images, images, images],axis=-1)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "datadir = \"/data/shared/dwerran/converted/\"\n",
    "num_train = 100  # Limit the number of images used in training to shorten epoch time\n",
    "\n",
    "train_files = glob.glob(os.path.join(datadir, 'train_file_*'))\n",
    "train_files = random.choice(train_files)  # Choose one of the training files to use (for now)\n",
    "\n",
    "# Open the chosen file and extract the dataset\n",
    "f = tables.open_file(train_files, 'r')\n",
    "a = np.array(f.root.img_pt) # Images\n",
    "b = np.array(f.root.label) # Labels\n",
    "# Randomly shuffle label and images, keep the indexing\n",
    "c = np.c_[a.reshape(len(a), -1), b.reshape(len(b), -1)]\n",
    "np.random.shuffle(c)\n",
    "train_images = c[:, :a.size//len(a)].reshape(a.shape)\n",
    "train_labels = c[:, a.size//len(a):].reshape(b.shape)\n",
    "\n",
    "# Limit the data set to make the notebook execute quickly.\n",
    "train_images = train_images[:num_train]\n",
    "train_labels = train_labels[:num_train]\n",
    "\n",
    "train_images = normalize_and_rgb(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 100\n",
    "\n",
    "test_files = glob.glob(os.path.join(datadir, 'test/test_file_*'))\n",
    "test_files = random.choice(test_files)  # Choose one of the training files to use (for now)\n",
    "\n",
    "# Open the chosen file and extract the dataset\n",
    "f = tables.open_file(test_files, 'r')\n",
    "a = np.array(f.root.img_pt) # Images\n",
    "b = np.array(f.root.label) # Labels\n",
    "# Randomly shuffle label and images, keep the indexing\n",
    "c = np.c_[a.reshape(len(a), -1), b.reshape(len(b), -1)]\n",
    "np.random.shuffle(c)\n",
    "test_images = c[:, :a.size//len(a)].reshape(a.shape)\n",
    "test_labels = c[:, a.size//len(a):].reshape(b.shape)\n",
    "\n",
    "# Limit the data set to make the notebook execute quickly.\n",
    "test_images = test_images[:num_test]\n",
    "test_labels = test_labels[:num_test]\n",
    "\n",
    "test_images = normalize_and_rgb(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model\n",
    "We use ResNet50 for the featuirzer and build our own classifier using Keras layers. We train the featurizer and the classifier as one model. The weights trained on ImageNet are used as the starting point for the retraining of our featurizer. The weights are loaded from tensorflow checkpoint files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing image dataset to the ResNet50 featurizer, we need to preprocess the input file to get it into the form expected by ResNet50. ResNet50 expects float tensors representing the images in BGR, channel last order. Given that our images are greyscale, this isn't relevant to us, as we will simply be copying the data in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.contrib.brainwave.models.utils as utils\n",
    "\n",
    "def preprocess_images():\n",
    "    # Create a placeholder for our incoming images\n",
    "    in_images = tf.placeholder(tf.float32)\n",
    "    in_height = 64\n",
    "    in_width = 64\n",
    "    in_images.set_shape([None, in_height, in_width, 3])\n",
    "    \n",
    "    # Resize those images to fit our featurizer\n",
    "    out_width = 224\n",
    "    out_height = 224\n",
    "    image_tensors = tf.image.resize_images(in_images, [out_height,out_width])\n",
    "    image_tensors = tf.to_float(image_tensors)\n",
    "    \n",
    "    return in_images, image_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Keras layer APIs to construct the classifier. Because we're using the tensorflow backend, we can train this classifier in one session with our Resnet50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_classifier(in_tensor):\n",
    "    from keras.layers import Dropout, Dense, Flatten\n",
    "    K.set_session(tf.get_default_session())\n",
    "    \n",
    "    FC_SIZE = 1024\n",
    "    NUM_CLASSES = 2\n",
    "\n",
    "    x = Dropout(0.2, input_shape=(1, 1, 2048,))(in_tensor)\n",
    "    x = Dense(FC_SIZE, activation='relu', input_dim=(1, 1, 2048,))(x)\n",
    "    x = Flatten()(x)\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax', input_dim=FC_SIZE, name='classifier_output')(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every component of the model is defined, we can construct the model. Constructing the model with the project brainwave models is two steps - first we import the graph definition, then we restore the weights of the model into a tensorflow session. Because the quantized graph defintion and the float32 graph defintion share the same node names in the graph definitions, we can initally train the weights in float32, and then reload them with the quantized operations (which take longer) to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(quantized, starting_weights_directory = None):\n",
    "    from azureml.contrib.brainwave.models import Resnet50, QuantizedResnet50\n",
    "    \n",
    "    # Convert images to 3D tensors [width,height,channel]\n",
    "    in_images, image_tensors = preprocess_images()\n",
    "\n",
    "    # Construct featurizer using quantized or unquantized ResNet50 model\n",
    "    if not quantized:\n",
    "        featurizer = Resnet50(saved_model_dir)\n",
    "    else:\n",
    "        featurizer = QuantizedResnet50(saved_model_dir, custom_weights_directory = starting_weights_directory)\n",
    "\n",
    "\n",
    "    features = featurizer.import_graph_def(input_tensor=image_tensors)\n",
    "    # Construct classifier\n",
    "    preds = construct_classifier(features)\n",
    "\n",
    "    # Initialize weights\n",
    "    sess = tf.get_default_session()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    featurizer.restore_weights(sess)\n",
    "\n",
    "    return in_images, image_tensors, features, preds, featurizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "First we train the model with custom weights but without quantization. Training is done with native float precision (32-bit floats). We load the traing data set and batch the training with 10 epochs. When the performance reaches desired level or starts decredation, we stop the training iteration and save the weights as tensorflow checkpoint files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(preds, in_images, train_images, train_labels, is_retrain = False, train_epoch = 10):\n",
    "    \"\"\" training model \"\"\"\n",
    "    from keras.objectives import binary_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    learning_rate = 0.001 if is_retrain else 0.01\n",
    "        \n",
    "    # Specify the loss function\n",
    "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))   \n",
    "    cross_entropy = tf.reduce_mean(binary_crossentropy(in_labels, preds))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    accuracy = tf.reduce_mean(categorical_accuracy(in_labels, preds))\n",
    "    auc = tf.metrics.auc(tf.cast(in_labels, tf.bool), preds)\n",
    "    \n",
    "    def chunks(a, b, n):\n",
    "        \"\"\"Yield successive n-sized chunks from a and b.\"\"\"\n",
    "        for i in range(0, num_train, n):\n",
    "            yield a[i:i + n], b[i:i + n]\n",
    "\n",
    "    chunk_size = 16\n",
    "    chunk_num = len(train_labels) / chunk_size\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "    \n",
    "    loss_over_epoch = []\n",
    "    accuracy_over_epoch = []\n",
    "    auc_over_epoch = []\n",
    "    \n",
    "    for epoch in range(train_epoch):\n",
    "        avg_loss = 0\n",
    "        avg_accuracy = 0\n",
    "        avg_auc = 0\n",
    "        for img_chunk, label_chunk in tqdm(chunks(train_images, train_labels, chunk_size)):\n",
    "            _, loss = sess.run([optimizer, cross_entropy],\n",
    "                                feed_dict={in_images: img_chunk,\n",
    "                                           in_labels: label_chunk,\n",
    "                                           K.learning_phase(): 1})\n",
    "            avg_loss += loss / chunk_num\n",
    "            accuracy_result, auc_result = sess.run([accuracy, auc],\n",
    "                                     feed_dict={in_images: test_images,\n",
    "                                                in_labels: test_labels,\n",
    "                                                K.learning_phase(): 0})\n",
    "            avg_accuracy += accuracy_result / chunk_num\n",
    "            avg_auc += auc_result[1] / chunk_num\n",
    "        print(\"Epoch:\", (epoch + 1), \"loss = \", \"{:.3f}\".format(avg_loss))\n",
    "        print(\"Accuracy:\", \"{:.3f}\".format(avg_accuracy), \", Area under ROC curve:\", \"{:.3f}\".format(avg_auc))\n",
    "            \n",
    "        loss_over_epoch.append(avg_loss)\n",
    "        accuracy_over_epoch.append(avg_accuracy)\n",
    "        auc_over_epoch.append(avg_auc)\n",
    "            \n",
    "        # Reach desired performance\n",
    "        if (avg_loss < 0.001):\n",
    "            break\n",
    "            \n",
    "    return loss_over_epoch, accuracy_over_epoch, auc_over_epoch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(preds, in_images, test_images, test_labels):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    from keras.metrics import categorical_accuracy\n",
    "\n",
    "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    accuracy = tf.reduce_mean(categorical_accuracy(in_labels, preds))\n",
    "    auc = tf.metrics.auc(tf.cast(in_labels, tf.bool), preds)\n",
    "    \n",
    "    sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "    accuracy, auc = sess.run([accuracy, auc],\n",
    "                    feed_dict={in_images: test_images,\n",
    "                               in_labels: test_labels,\n",
    "                               K.learning_phase(): 0})\n",
    "    \n",
    "    return accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training currently leverages a hack to work around some apparent limits in the BW API. I have attempted to specify a custom weights directory when calling the `Resnet50` function in `construct_model()` above in the same way it is specified for `Quantized_Resnet50`. However, this throws an error, and since there is no API documentation yet, the way I'm working around it is rewriting our trained weights to the saved model directory. I will be reaching out to the team on this topic to see if they have a better suggestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /data/shared/dwerran/custom-weights-retrain/models/rn50/1.1.3/resnet50_bw\n",
      "Epoch: 1 loss =  1.850\n",
      "Accuracy: 0.674 , Area under ROC curve: 0.637\n",
      "Epoch: 2 loss =  1.847\n",
      "Accuracy: 0.717 , Area under ROC curve: 0.703\n",
      "Epoch: 3 loss =  1.292\n",
      "Accuracy: 0.747 , Area under ROC curve: 0.737\n",
      "Epoch: 4 loss =  0.815\n",
      "Accuracy: 0.768 , Area under ROC curve: 0.766\n",
      "Epoch: 5 loss =  0.769\n",
      "Accuracy: 0.810 , Area under ROC curve: 0.785\n",
      "Epoch: 6 loss =  0.617\n",
      "Accuracy: 0.861 , Area under ROC curve: 0.805\n",
      "Epoch: 7 loss =  0.478\n",
      "Accuracy: 0.882 , Area under ROC curve: 0.826\n",
      "Epoch: 8 loss =  0.471\n",
      "Accuracy: 0.880 , Area under ROC curve: 0.842\n",
      "Epoch: 9 loss =  0.378\n",
      "Accuracy: 0.894 , Area under ROC curve: 0.857\n",
      "Epoch: 10 loss =  0.314\n",
      "Accuracy: 0.898 , Area under ROC curve: 0.870\n",
      "Accuracy: 0.26 , Area under ROC curve: (0.0, 0.19714999)\n"
     ]
    }
   ],
   "source": [
    "# Launch the training\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "num_epoch_train = 10\n",
    "\n",
    "with sess.as_default():\n",
    "    in_images, image_tensors, features, preds, featurizer = construct_model(quantized=False)\n",
    "    loss_over_epoch, accuracy_over_epoch, auc_over_epoch = \\\n",
    "        train_model(preds, in_images, train_images, train_labels, is_retrain=False, train_epoch=num_epoch_train)    \n",
    "    accuracy, auc = test_model(preds, in_images, test_images, test_labels)  \n",
    "    # print(\"Accuracy:\", accuracy, \", Area under ROC curve:\", auc)\n",
    "    featurizer.save_weights(saved_model_dir + \"/rn50/1.1.3/resnet50_bw\", tf.get_default_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "After training, we evaluate the trained model's accuracy on test dataset with quantization. So that we know the model's performance if it is deployed on the FPGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained model with quantization\n",
      "INFO:tensorflow:Restoring parameters from /data/shared/dwerran/custom-weights-retrain/models/rn50/1.1.3/resnet50_bw\n",
      "Accuracy: 0.55 , Area under ROC curve: (0.0, 0.5998)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Testing trained model with quantization\")\n",
    "    in_images, image_tensors, features, preds, quantized_featurizer = construct_model(quantized=True, starting_weights_directory=saved_model_dir + \"/rn50/1.1.3/\")\n",
    "    accuracy, auc = test_model(preds, in_images, test_images, test_labels)      \n",
    "    print(\"Accuracy:\", accuracy, \", Area under ROC curve:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Model\n",
    "Sometimes, the model's accuracy can drop significantly after quantization. In those cases, we need to retrain the model enabled with quantization to get better model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with quantization\n",
      "Epoch: 1 loss =  1.191\n",
      "Accuracy: 0.610 , Area under ROC curve: 0.612\n",
      "Epoch: 2 loss =  0.771\n",
      "Accuracy: 0.766 , Area under ROC curve: 0.722\n",
      "Epoch: 3 loss =  0.830\n",
      "Accuracy: 0.754 , Area under ROC curve: 0.767\n",
      "Epoch: 4 loss =  0.750\n",
      "Accuracy: 0.790 , Area under ROC curve: 0.794\n",
      "Epoch: 5 loss =  0.802\n",
      "Accuracy: 0.795 , Area under ROC curve: 0.807\n",
      "Epoch: 6 loss =  0.744\n",
      "Accuracy: 0.805 , Area under ROC curve: 0.821\n",
      "Epoch: 7 loss =  0.742\n",
      "Accuracy: 0.818 , Area under ROC curve: 0.833\n",
      "Epoch: 8 loss =  0.655\n",
      "Accuracy: 0.821 , Area under ROC curve: 0.842\n",
      "Epoch: 9 loss =  0.781\n",
      "Accuracy: 0.829 , Area under ROC curve: 0.850\n",
      "Epoch: 10 loss =  0.617\n",
      "Accuracy: 0.814 , Area under ROC curve: 0.857\n",
      "Accuracy: (0.47, (0.0, 0.5998))\n"
     ]
    }
   ],
   "source": [
    "# while (accuracy < 0.93):  # This replaces \"if (accuracy...)\" in the original notebook\n",
    "\n",
    "num_epoch_finetune = 10\n",
    "\n",
    "if (accuracy < 0.93):\n",
    "    with sess.as_default():\n",
    "        print(\"Fine-tuning model with quantization\")\n",
    "        loss_over_epoch_ft, accuracy_over_epoch_ft, auc_over_epoch_ft = \\\n",
    "            train_model(preds, in_images, train_images, train_labels, is_retrain=True, train_epoch=num_epoch_finetune)\n",
    "        accuracy = test_model(preds, in_images, test_images, test_labels)        \n",
    "        # print(\"Accuracy:\", accuracy)\n",
    "        featurizer.save_weights(saved_model_dir + \"/rn50/1.1.3/resnet50_bw\", tf.get_default_session())\n",
    "\n",
    "loss_over_epoch = loss_over_epoch + loss_over_epoch_ft\n",
    "accuracy_over_epoch = accuracy_over_epoch + accuracy_over_epoch_ft\n",
    "auc_over_epoch = auc_over_epoch + auc_over_epoch_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some plotting. We're going to take the data from the first training run and append it to the data from the second. You'll see a drop in the accuracy and an increase in loss at that time, but it should eventually return to nominal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8504117965698244, 1.8465952968597414, 1.2918612337112425, 0.8147461414337158, 0.7686095762252807, 0.616898856163025, 0.4780296659469605, 0.4713448238372802, 0.37776726603508, 1.1906987857818603, 0.7712285137176513, 0.8304007625579835, 0.7504903984069824, 0.8020374631881714, 0.7436089897155762, 0.7422701692581177, 0.6546333837509155, 0.7809856605529786, 0.6174936294555664]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (19,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-f0a64fcbb709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_over_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_over_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss over time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2811\u001b[0m     return gca().plot(\n\u001b[1;32m   2812\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2813\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (19,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create our x-axis, which should be integers only since we're counting epochs\n",
    "x = range(1, num_epoch_train + num_epoch_finetune + 1)\n",
    "\n",
    "print(loss_over_epoch[0:9] + loss_over_epoch[10])\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(x, loss_over_epoch[0:9] + loss_over_epoch[10])\n",
    "plt.title(\"Loss over time\")\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(x, accuracy_over_epoch[0:9] + accuracy_over_epoch[10])\n",
    "plt.title(\"Accuracy over time\")\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(x, auc_over_epoch[0:9] + auc_over_epoch[10])\n",
    "plt.title(\"Area under ROC curve over time\")\n",
    "plt.xticks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Definition\n",
    "Like in the QuickStart notebook our service definition pipeline consists of three stages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n",
      "INFO:tensorflow:Restoring parameters from /data/shared/dwerran/custom-weights-retrain/models/rn50/1.1.3/resnet50_bw\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n",
      "/data/shared/dwerran/custom-weights-retrain/models/model_def.zip\n"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.brainwave.pipeline import ModelDefinition, TensorflowStage, BrainWaveStage\n",
    "\n",
    "model_def_path = os.path.join(saved_model_dir, 'model_def.zip')\n",
    "\n",
    "model_def = ModelDefinition()\n",
    "model_def.pipeline.append(TensorflowStage(sess, in_images, image_tensors))\n",
    "model_def.pipeline.append(BrainWaveStage(sess, quantized_featurizer))\n",
    "model_def.pipeline.append(TensorflowStage(sess, features, preds))\n",
    "model_def.save(model_def_path)\n",
    "print(model_def_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "Go to our [GitHub repo](https://aka.ms/aml-real-time-ai) \"docs\" folder to learn how to create a Model Management Account and find the required information below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /nfshome/dwerran/MachineLearningNotebooks/aml_config/config.json\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time the code below runs it will create a new service running your model. If you want to change the model you can make changes above in this notebook and save a new service definition. Then this code will update the running service in place to run the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model top-transfer-resnet50-model\n",
      "Creating image\n",
      "Image creation operation finished for image modelbuild-service:8, operation \"Succeeded\"\n",
      "Creating service\n"
     ]
    },
    {
     "ename": "WebserviceException",
     "evalue": "Received bad response from Model Management Service:\nResponse Code: 504\nHeaders: {'Date': 'Fri, 14 Dec 2018 20:28:55 GMT', 'Content-Type': 'text/html', 'Content-Length': '176', 'Connection': 'keep-alive', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\nContent: b'<html>\\r\\n<head><title>504 Gateway Time-out</title></head>\\r\\n<body bgcolor=\"white\">\\r\\n<center><h1>504 Gateway Time-out</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8d90a09c520e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebservice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, workspace, name)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 raise WebserviceException('WebserviceNotFound: Webservice with name {} not found in provided '\n\u001b[0;32m---> 77\u001b[0;31m                                           'workspace'.format(name))\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebserviceException\u001b[0m: WebserviceNotFound: Webservice with name modelbuild-service not found in provided workspace",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36m_deploy_webservice\u001b[0;34m(workspace, name, webservice_payload, webservice_class)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmms_endpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwebservice_payload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 504 Server Error: Gateway Time-out for url: https://eastus2.modelmanagement.azureml.net/api/subscriptions/80defacd-509e-410c-9812-6e52ed6a0016/resourceGroups/CMS_FPGA_Resources/providers/Microsoft.MachineLearningServices/workspaces/Fermilab/services?api-version=2018-11-19",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8d90a09c520e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mregistered_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mdeploy_from_model\u001b[0;34m(workspace, name, models, image_config, deployment_config, deployment_target)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_state\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Succeeded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error occurred creating image {} for service.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mdeploy_from_image\u001b[0;34m(workspace, name, image, deployment_config, deployment_target)\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_webservice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ACI'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdeployment_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_webservice_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/azureml/contrib/brainwave/brainwave_webservice.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(workspace, name, image, deployment_config)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mcreate_payload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrainwaveWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_create_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deploy_webservice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_payload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBrainwaveWebservice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mWebserviceException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amlenv/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36m_deploy_webservice\u001b[0;34m(workspace, name, webservice_payload, webservice_class)\u001b[0m\n\u001b[1;32m    328\u001b[0m                                       \u001b[0;34m'Headers: {}\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                                       \u001b[0;34m'Content: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                                       resp.status_code)\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m202\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             raise WebserviceException('Error occurred creating service:\\n'\n",
      "\u001b[0;31mWebserviceException\u001b[0m: Received bad response from Model Management Service:\nResponse Code: 504\nHeaders: {'Date': 'Fri, 14 Dec 2018 20:28:55 GMT', 'Content-Type': 'text/html', 'Content-Length': '176', 'Connection': 'keep-alive', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\nContent: b'<html>\\r\\n<head><title>504 Gateway Time-out</title></head>\\r\\n<body bgcolor=\"white\">\\r\\n<center><h1>504 Gateway Time-out</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n'"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.core.image import Image\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.contrib.brainwave import BrainwaveWebservice, BrainwaveImage\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "model_name = \"top-transfer-resnet50-model\"\n",
    "image_name = \"top-transfer-resnet50-image\"\n",
    "service_name = \"modelbuild-service\"\n",
    "\n",
    "registered_model = Model.register(ws, model_def_path, model_name)\n",
    "\n",
    "image_config = BrainwaveImage.image_configuration()\n",
    "deployment_config = BrainwaveWebservice.deploy_configuration()\n",
    "    \n",
    "try:\n",
    "    service = Webservice(ws, service_name)\n",
    "    service.delete()\n",
    "    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n",
    "    service.wait_for_deployment(True)\n",
    "except WebserviceException:\n",
    "    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n",
    "    service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service is now running in Azure and ready to serve requests. We can check the address and port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.ip_address + ':' + str(service.port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client\n",
    "There is a simple test client at amlrealtimeai.PredictionClient which can be used for testing. We'll use this client to score an image with our new service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.brainwave.client import PredictionClient\n",
    "client = PredictionClient(service.ip_address, service.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adapt the client [code](../../pythonlib/amlrealtimeai/client.py) to meet your needs. There is also an example C# [client](../../sample-clients/csharp).\n",
    "\n",
    "The service provides an API that is compatible with TensorFlow Serving. There are instructions to download a sample client [here](https://www.tensorflow.org/serving/setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request\n",
    "Let's see how our service does on a few images. It may get a few wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an image to classify\n",
    "for i in range(10):\n",
    "    image_file = test_images[i]\n",
    "    label = test_labels[i]\n",
    "    results = client.score_image(image_file)\n",
    "    result = 'CORRECT ' if (results[0] > results[1]) == label[0] else 'WRONG '\n",
    "    print(result + str(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Run the cell below to delete your service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License for plot_confusion_matrix:\n",
    "\n",
    "New BSD License\n",
    "\n",
    "Copyright (c) 2007-2018 The scikit-learn developers.\n",
    "All rights reserved.\n",
    "\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "  a. Redistributions of source code must retain the above copyright notice,\n",
    "     this list of conditions and the following disclaimer.\n",
    "  b. Redistributions in binary form must reproduce the above copyright\n",
    "     notice, this list of conditions and the following disclaimer in the\n",
    "     documentation and/or other materials provided with the distribution.\n",
    "  c. Neither the name of the Scikit-learn Developers  nor the names of\n",
    "     its contributors may be used to endorse or promote products\n",
    "     derived from this software without specific prior written\n",
    "     permission. \n",
    "\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR\n",
    "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n",
    "LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n",
    "OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n",
    "DAMAGE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "coverste"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
